{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "571cf802",
   "metadata": {},
   "source": [
    "# Document Splitting\n",
    "<img src=\"./images/L2-Document_splitting.png\" alt=\"workflow\" width=\"500\"/>\n",
    "\n",
    "<img src=\"./images/L2_textsplitter.png\" alt=\"methods\" width=\"500\"/>\r\n",
    "\n",
    "- long document into smaller chunks that can **fit** into your **model's context window**\n",
    "- built-in document transformers that make it easy to **split, combine, filter**, and otherwise **manipulate documents**. Packages -- `**langchain-text-splitters**`\n",
    "- keep the semantically related pieces of text together\n",
    "\n",
    "1. How the text is split\r",
    "2. \n",
    "How the chunk size is measure\n",
    "\n",
    "- Split the text up into small, semantically meaningful chunks (often sentences).\r",
    "- \n",
    "Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function)\n",
    "- \r\n",
    "Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "23f498fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\abhin\\anaconda3\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langchain) (0.1.120)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langchain) (2.9.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain) (2.1)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\abhin\\anaconda3\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langchain-community) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langchain-community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.0 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langchain-community) (0.1.120)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langchain-community) (2.5.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langchain<0.4.0,>=0.3.0->langchain-community) (0.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langchain<0.4.0,>=0.3.0->langchain-community) (2.9.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-community) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-community) (3.10.7)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain-community) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain-community) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain-community) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.0->langchain-community) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.0->langchain-community) (2.23.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\abhin\\anaconda3\\lib\\site-packages (1.24.10)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.10 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from pymupdf) (1.24.10)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\abhin\\anaconda3\\lib\\site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.26.0->tiktoken) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhin\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Downloading tiktoken-0.7.0-cp312-cp312-win_amd64.whl (799 kB)\n",
      "   ---------------------------------------- 0.0/799.3 kB ? eta -:--:--\n",
      "   - ------------------------------------- 30.7/799.3 kB 660.6 kB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 92.2/799.3 kB 1.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 225.3/799.3 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 450.6/799.3 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 634.9/799.3 kB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 747.5/799.3 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 799.3/799.3 kB 2.8 MB/s eta 0:00:00\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.7.0\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain\n",
    "!pip install -U langchain-community\n",
    "!pip install pymupdf\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "30c7fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter,CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f77771-298e-4383-91b9-8930ed2dff3e",
   "metadata": {},
   "source": [
    "## RecursiveCharacterSplitter\n",
    "- `split-text`\n",
    "- `create_documents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a7339b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split:  ['12', '123', '4567', '6789', '1234', '3456', '1234', '34567', '6789']\n",
      "['12->2', '123->3', '4567->4', '6789->4', '1234->4', '3456->4', '1234->4', '34567->5', '6789->4']\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=5,\n",
    "    chunk_overlap=2,\n",
    "    separators=[\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \"\",\n",
    "        #---- Set for different language-----\n",
    "        # \".\",\n",
    "        # \",\",\n",
    "        # \"\\u200b\",  # Zero-width space\n",
    "        # \"\\uff0c\",  # Fullwidth comma\n",
    "        # \"\\u3001\",  # Ideographic comma\n",
    "        # \"\\uff0e\",  # Fullwidth full stop\n",
    "        # \"\\u3002\",  # Ideographic full stop    \n",
    "    ],\n",
    "    length_function=len,\n",
    ")\n",
    "text1 = '12\\n\\n123 456789\\n123456\\n\\n123456789'\n",
    "\n",
    "split_list = text_splitter.split_text(text1)\n",
    "print(\"After Split: \",split_list)\n",
    "for i in range(len(split_list)):\n",
    "    split_list[i] = split_list[i]+\"->\"+str(len(split_list[i])) \n",
    "print(split_list)\n",
    "\n",
    "text2 = text_splitter.create_documents([text1])\n",
    "# text2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccaa90d-291f-40b7-81cc-1fa98b20cb0f",
   "metadata": {},
   "source": [
    "## CharacterTextSplitter   \n",
    "- not considering multiple levels of splitting criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "112e208b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 11, which is longer than the specified 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split:  ['12345  6789', '123456789123456789123456789\\n123456789']\n",
      "['12345  6789->11', '123456789123456789123456789\\n123456789->37']\n"
     ]
    }
   ],
   "source": [
    "text4 = '''12345  6789\\n\\n123456789123456789123456789\\n123456789 '''\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=1,\n",
    "    length_function= len,\n",
    "    separator = '\\n\\n'\n",
    ")\n",
    "split_list2 = c_splitter.split_text(text4)\n",
    "print(\"After Split: \",split_list2)\n",
    "for i in range(len(split_list2)):\n",
    "    split_list2[i] = split_list2[i]+\"->\"+str(len(split_list2[i])) \n",
    "print(split_list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f0890",
   "metadata": {},
   "source": [
    "Let's reduce the chunk size a bit and add a period to our separators:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b8cd92d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# Specify the path to your PDF file\n",
    "pdf_file_path = \"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"\n",
    "\n",
    "# Load the PDF document\n",
    "loader = PyMuPDFLoader(pdf_file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# # Now you can use the documents in LangChain\n",
    "# for doc in documents:\n",
    "#     print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c286fa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2097d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "cb0eb13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "618ebb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "loader = NotionDirectoryLoader(\"docs/Notion_DB\")\n",
    "notion_db = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "25c93697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(notion_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "ac709181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d004f",
   "metadata": {},
   "source": [
    "## Token splitting\n",
    "\n",
    "We can also split on token count explicity, if we want.\n",
    "\n",
    "This can be useful because LLMs often have context windows designated in tokens.\n",
    "\n",
    "Tokens are often ~4 characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "b2ac9766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foo', ' bar', ' b', 'az', 'zy', 'foo']"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n",
    "text1 = \"foo bar bazzyfoo\"\n",
    "text_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "bb51f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "1b9ffb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'file_path': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''}, page_content='Machine')"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82a84448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648df746",
   "metadata": {},
   "source": [
    "## Context aware splitting\n",
    "\n",
    "Chunking aims to keep text with common context together.\n",
    "\n",
    "A text splitting often uses sentences or other delimiters to keep related text together but many documents (such as Markdown) have structure (headers) that can be explicitly used in splitting.\n",
    "\n",
    "We can use MarkdownHeaderTextSplitter to preserve header metadata in our chunks, as show below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "d0b4fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "fdb6db95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 3)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_document = \"\"\"# Title\\n\\n \\\n",
    "## Chapter 1\\n\\n \\\n",
    "Hi this is Jim\\n\\n Hi this is Joe\\n\\n \\\n",
    "### Section \\n\\n \\\n",
    "Hi this is Lance \\n\\n \n",
    "## Chapter 2\\n\\n \\\n",
    "Hi this is Molly\"\"\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "type(md_header_splits),len(md_header_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "e771191d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Header 1': 'Title', 'Header 2': 'Chapter 1'}, page_content='Hi this is Jim  \\nHi this is Joe')"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_header_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "391ae033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Header 1': 'Title', 'Header 2': 'Chapter 1', 'Header 3': 'Section'}, page_content='Hi this is Lance')"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_header_splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "76de0869",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = NotionDirectoryLoader(\"docs/Notion_DB\")\n",
    "docs = loader.load()\n",
    "txt = ' '.join([d.page_content for d in docs])\n",
    "# txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ccf84da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c89d678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_header_splits = markdown_splitter.split_text(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33c7c438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"We kick off with the practical side of things and then dig into the idea behind it.  \\n- **How *time off* works at Blendle**\\n- Time off is about the time you **need,** not about a **quota.**\\n- At Blendle, **HR doesn't keep track** of your holidays **and we don't 'pay out' at the end of the ride.** When in doubt: 4-6 weeks is a good bandwidth. Less than that is not enough, more than that can happen, just check with your lead if you're in doubt if it's reasonable.\\n- **We stick to the commonly used national holidays**, which comes down to ~8 days per year. We are a startup and there are teams who have work to be done 24/7. We don't like being told whether we are off or not on Eid al-Fitr (Suikerfeest, ending of Ramadan) by some rules someone made up, so this is a guideline we use: feel free to work or be off when you want.\\n- Make sure to **take enough moments of rest when you have periods of working hard**. For example: worked a few nights to finish a project? Go home at 12:00 the next Monday to have lunch with a friend and go to the gym (or have a New Girl-marathon). It's about balance and flexibility.\\n- You agree on your time off with your team(lead).\\n- You put your time off in the agenda: add it to your own and all@ agenda.\\n- A life event occurs: communicate what you need and check with your team(lead) when in doubt. 'Hey I just had a baby, won't be at work this week, see you next week. His name is Jan by the way!'.\\n- Enjoy: don't worry if you go to the gym or get a haircut during classic 'work hours'. We trust you to make the best of your days for Blendle.\\n- **This only works when**\\n- Flexibility goes both ways.\\n- You make sure to take enough time off: since you don't get paid at the end of the ride.\\n- You communicate in a crystal clear manner: agreeing the dates with others and putting it in the agenda.\\n- **Backdrop**  \\nThe law and additional ruling tell us what we can and can not do according to time off. We believe that is a very limited way of thinking about time off. We want you to think about what you actually need. Starting with two important pillars:  \\n- We work hard, we need time to reload and rest. Sometimes we work harder, so we need more. Some times we work less, so we need less.\\n- We have something called 'life' happening which means we can't work because we have more important stuff to do.  \\nWe want you to take enough rest to reload and recharge so you can be at your prime when working. Taking enough rest is as important as giving it your all when working. The well known on and off switch.  \\nThe concept of work isn't always caught in time (40 hours) and place. Most of the people at Blendle don't keep track of time because time is such a limited variable to grasp the concept of work. There is a balance to strike here and just looking at hours or a holiday quota limits this discussion.  \\n**That is why we don't keep track of holidays.**  \\nOur people don't have a holiday quota, but a moral obligation to their loved ones and Blendle to work hard and take enough time to rest. Alright, sounds heavy. How does that work?  \\n'Time off' can be divided into two buckets:  \\n1. **Holidays and national holidays**:  \\nYour contract states you have 4 weeks of paid time off per year, by law.  \\nOn average we have 8 national holidays per year. We have no CAO (collective employment agreement), which means you are not entitled to paid time off on every national holiday. We decided to follow the most common situation in an average company, which in practice means most people are off on the big national holidays. But that's your own choice, it's not a right or an obligation, so feel free to improvise. Some teams even have to (editorial and support, for example).  \\nThese rules state that we are off on 'Bevrijdingsdag' once every 5 years. But we aren't allowed time off on Eid al-Fitr (Suikerfeest, ending of the Ramadan). That makes no sense to us.  \\nSo here is how we approach holidays and national holidays in practice.  \\nYou 'have' 5 weeks holiday and ~8 national holidays per year. We want you to use this time off to reload but you decide when and how to consume this. You must also feel free to improvise. You work hard, maybe even a bit too hard, so you might need some extra time off to stay happy and healthy. This also works the other way around: maybe you had a chill quarter and feel no need to take a holiday: feel free to do so. We want you to think about what your body and mind needs instead of your quota.  \\n2. **Life events:**  \\nLife happens. Your cat dies, your car broke, you move to another city, you get married or have kids. Life events which all need your attention. We decided that we are not going to decide for you how much time you **get** for certain events. The question is: how much time do you actually **need.**  \\nMost companies don't give you time off for funerals that are not for immediate relatives. We think we don't need a rule to tell us if that funeral is important for you. So take the time you need.  \\n- Edge cases\\n- Sabbatical: check with HR if you for example have plans to take a very long trip (longer then ~4 weeks).\",\n",
       " 'metadata': {'Header 1': 'Time off: holidays and national holidays'}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_header_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca2850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
